{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21cf3c3b",
   "metadata": {},
   "source": [
    "# SymbolicGym: Integration with Popular RL Libraries\n",
    "\n",
    "This notebook demonstrates how to integrate SymbolicGym with popular reinforcement learning libraries:\n",
    "1. Stable Baselines3\n",
    "2. RLlib (Ray)\n",
    "3. Custom PyTorch implementation\n",
    "\n",
    "We'll solve the same SAT problems with each approach to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88433e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if symbolicgym is installed\n",
    "try:\n",
    "    import symbolicgym\n",
    "\n",
    "    print(f\"Using SymbolicGym version: {getattr(symbolicgym, '__version__', 'dev')}\")\n",
    "except ImportError:\n",
    "    print(\"SymbolicGym not found. Please install with: pip install -e .\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46851b",
   "metadata": {},
   "source": [
    "## 1. Creating a SAT Problem\n",
    "\n",
    "First, we'll create a standard SAT problem to use with all RL frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_formula(difficulty=\"medium\"):\n",
    "    \"\"\"Create a SAT formula with specified difficulty.\"\"\"\n",
    "    if difficulty == \"easy\":\n",
    "        # Simple 3-SAT with 5 variables\n",
    "        return {\n",
    "            \"clauses\": [\n",
    "                [1, 2, 3],\n",
    "                [-1, -2, 4],\n",
    "                [2, -3, 5],\n",
    "                [-2, 3, -4],\n",
    "                [1, -3, -5],\n",
    "                [-1, 4, 5],\n",
    "                [1, -4, -5],\n",
    "                [-1, -2, -3],\n",
    "                [2, 4, -5],\n",
    "                [3, -4, 5],\n",
    "                [-1, 3, 4],\n",
    "                [1, 2, -5],\n",
    "            ],\n",
    "            \"num_vars\": 5,\n",
    "            \"name\": \"easy_3sat\",\n",
    "        }\n",
    "    elif difficulty == \"medium\":\n",
    "        # Generate random 3-SAT with 10 variables\n",
    "        clauses = []\n",
    "        num_vars = 10\n",
    "        num_clauses = 42  # ratio 4.2\n",
    "\n",
    "        for _ in range(num_clauses):\n",
    "            vars_in_clause = np.random.choice(num_vars, 3, replace=False) + 1\n",
    "            literals = [v if np.random.random() > 0.5 else -v for v in vars_in_clause]\n",
    "            clauses.append(literals)\n",
    "\n",
    "        return {\"clauses\": clauses, \"num_vars\": num_vars, \"name\": \"medium_3sat\"}\n",
    "    else:  # hard\n",
    "        # Generate random 3-SAT with 20 variables near phase transition\n",
    "        clauses = []\n",
    "        num_vars = 20\n",
    "        num_clauses = 85  # ratio 4.25\n",
    "\n",
    "        for _ in range(num_clauses):\n",
    "            vars_in_clause = np.random.choice(num_vars, 3, replace=False) + 1\n",
    "            literals = [v if np.random.random() > 0.5 else -v for v in vars_in_clause]\n",
    "            clauses.append(literals)\n",
    "\n",
    "        return {\"clauses\": clauses, \"num_vars\": num_vars, \"name\": \"hard_3sat\"}\n",
    "\n",
    "\n",
    "# Create our test formula\n",
    "formula = create_test_formula(\"medium\")\n",
    "print(\n",
    "    f\"Created {formula['name']} with {formula['num_vars']} variables and {len(formula['clauses'])} clauses\"\n",
    ")\n",
    "\n",
    "# Display a few sample clauses\n",
    "print(\"\\nSample clauses:\")\n",
    "for i in range(min(5, len(formula[\"clauses\"]))):\n",
    "    print(f\"Clause {i + 1}: {formula['clauses'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d42b2",
   "metadata": {},
   "source": [
    "## 2. State Preprocessing\n",
    "\n",
    "To use SymbolicGym with RL libraries, we need to convert the dictionary observations to flat vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observation):\n",
    "    \"\"\"Preprocess the SymbolicGym observation into a flat vector for RL algorithms.\"\"\"\n",
    "    variables = observation[\"variables\"]\n",
    "    clauses = observation[\"clauses\"]\n",
    "\n",
    "    # You can add more features here if desired\n",
    "    state = np.concatenate([variables, clauses])\n",
    "    return state\n",
    "\n",
    "\n",
    "# Create environment and test preprocessing\n",
    "from symbolicgym.envs.sat import SymbolicSatEnv\n",
    "\n",
    "env = SymbolicSatEnv(formula=formula, reward_mode=\"dense\", max_steps=100)\n",
    "obs, _ = env.reset(seed=RANDOM_SEED)\n",
    "\n",
    "# Show raw observation\n",
    "print(\"Raw observation:\")\n",
    "print(f\"- variables: shape {obs['variables'].shape}, values: {obs['variables']}\")\n",
    "print(f\"- clauses: shape {obs['clauses'].shape}, values: {obs['clauses']}\")\n",
    "\n",
    "# Show preprocessed observation\n",
    "preprocessed = preprocess_observation(obs)\n",
    "print(f\"\\nPreprocessed state: shape {preprocessed.shape}, values: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5015c1",
   "metadata": {},
   "source": [
    "## 3. Implementation with Custom PyTorch DQN\n",
    "\n",
    "First, let's implement a solution using a custom PyTorch DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        hidden_size=64,\n",
    "        lr=0.001,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.9,\n",
    "        epsilon_min=0.05,\n",
    "        epsilon_decay=0.995,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Initialize device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize networks\n",
    "        self.policy_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "\n",
    "        # Training metrics\n",
    "        self.rewards_history = []\n",
    "        self.loss_history = []\n",
    "        self.satisfaction_history = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.max(1)[1].item()\n",
    "        else:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return 0\n",
    "\n",
    "        # Sample transitions\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        batch = list(zip(*transitions, strict=False))\n",
    "\n",
    "        # Extract batch components\n",
    "        state_batch = torch.FloatTensor(batch[0]).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch[1]).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch[2]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(batch[3]).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch[4]).to(self.device)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def train(self, env, num_episodes, batch_size=64, target_update=10):\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            obs, _ = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "            # Initialize episode variables\n",
    "            total_reward = 0\n",
    "            max_satisfaction = 0\n",
    "            episode_loss = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                steps += 1\n",
    "\n",
    "                # Select and perform action\n",
    "                action = self.select_action(state)\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "                next_state = preprocess_observation(next_obs)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Store transition\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Track max satisfaction\n",
    "                if \"satisfaction_ratio\" in info:\n",
    "                    max_satisfaction = max(max_satisfaction, info[\"satisfaction_ratio\"])\n",
    "\n",
    "                # Learn from experience\n",
    "                loss = self.learn(batch_size)\n",
    "                if loss:\n",
    "                    episode_loss += loss\n",
    "\n",
    "            # Update target network\n",
    "            if episode % target_update == 0:\n",
    "                self.update_target_net()\n",
    "\n",
    "            # Decay exploration rate\n",
    "            self.decay_epsilon()\n",
    "\n",
    "            # Store metrics\n",
    "            self.rewards_history.append(total_reward)\n",
    "            self.loss_history.append(episode_loss / max(1, steps))\n",
    "            self.satisfaction_history.append(max_satisfaction)\n",
    "\n",
    "            # Print progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "                    f\"Reward: {total_reward:.2f}, \"\n",
    "                    f\"Loss: {episode_loss / max(1, steps):.4f}, \"\n",
    "                    f\"Satisfaction: {max_satisfaction:.2f}, \"\n",
    "                    f\"Epsilon: {self.epsilon:.2f}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad0ac7",
   "metadata": {},
   "source": [
    "Now let's train our custom DQN agent on the SAT problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1964a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment for custom DQN\n",
    "env_custom = SymbolicSatEnv(formula=formula, reward_mode=\"dense\", max_steps=100)\n",
    "\n",
    "# Get state and action dimensions\n",
    "obs, _ = env_custom.reset(seed=RANDOM_SEED)\n",
    "state = preprocess_observation(obs)\n",
    "state_size = len(state)\n",
    "action_size = env_custom.action_space.n\n",
    "\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "\n",
    "# Create and train the agent\n",
    "custom_agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    hidden_size=64,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.9,\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995,\n",
    ")\n",
    "\n",
    "print(\"Training custom PyTorch DQN agent...\")\n",
    "custom_agent.train(env_custom, num_episodes=100, batch_size=64, target_update=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e3190",
   "metadata": {},
   "source": [
    "Let's visualize the training results for our custom DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee45118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(custom_agent.rewards_history)\n",
    "plt.title(\"Custom DQN: Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(custom_agent.loss_history)\n",
    "plt.title(\"Custom DQN: Training Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot satisfaction\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(custom_agent.satisfaction_history)\n",
    "plt.title(\"Custom DQN: Max Clause Satisfaction\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Satisfaction Ratio\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c9c1c",
   "metadata": {},
   "source": [
    "## 4. Implementation with Stable Baselines3\n",
    "\n",
    "Now let's train an agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/), a popular RL library.\n",
    "\n",
    "First, we need to create a wrapper to convert our dictionary observations to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Stable-Baselines3 is installed\n",
    "try:\n",
    "    import stable_baselines3\n",
    "    from stable_baselines3 import DQN as SB3_DQN\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "    print(f\"Using Stable-Baselines3 version: {stable_baselines3.__version__}\")\n",
    "    sb3_available = True\n",
    "except ImportError:\n",
    "    print(\"Stable-Baselines3 not found. Install with: pip install stable-baselines3\")\n",
    "    sb3_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sb3_available:\n",
    "    # Create environment wrapper for SB3\n",
    "    class SATGymWrapper(gym.Wrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            # Get observation dimensions\n",
    "            obs, _ = env.reset()\n",
    "            processed_obs = preprocess_observation(obs)\n",
    "\n",
    "            # Define new observation space\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=-1.0, high=1.0, shape=(len(processed_obs),), dtype=np.float32\n",
    "            )\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "            return preprocess_observation(obs), info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            processed_obs = preprocess_observation(obs)\n",
    "            return processed_obs, reward, terminated, truncated, info\n",
    "\n",
    "    # Create and wrap environment\n",
    "    raw_env = SymbolicSatEnv(formula=formula, reward_mode=\"dense\", max_steps=100)\n",
    "    env_sb3 = SATGymWrapper(raw_env)\n",
    "\n",
    "    # Convert to VecEnv (SB3 format)\n",
    "    vec_env = DummyVecEnv([lambda: env_sb3])\n",
    "\n",
    "    print(\"Training Stable-Baselines3 DQN agent...\")\n",
    "\n",
    "    # Create and train SB3 agent\n",
    "    model = SB3_DQN(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        learning_rate=0.001,\n",
    "        buffer_size=10000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        exploration_fraction=0.3,\n",
    "        exploration_initial_eps=0.9,\n",
    "        exploration_final_eps=0.05,\n",
    "        train_freq=1,\n",
    "        target_update_interval=10,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f34a0e",
   "metadata": {},
   "source": [
    "## 5. Implementation with RLlib (Ray)\n",
    "\n",
    "Finally, let's implement a solution using [RLlib](https://docs.ray.io/en/latest/rllib/index.html), which is powerful for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ray RLlib is installed\n",
    "try:\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "    print(f\"Using Ray version: {ray.__version__}\")\n",
    "    rllib_available = True\n",
    "except ImportError:\n",
    "    print(\"Ray RLlib not found. Install with: pip install ray[rllib]\")\n",
    "    rllib_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a25635",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rllib_available:\n",
    "    # Create environment wrapper for RLlib\n",
    "    class SATGymWrapperRLlib(gym.Wrapper):\n",
    "        def __init__(self, env_config=None):\n",
    "            env_config = env_config or {}\n",
    "            formula_to_use = env_config.get(\"formula\", formula)\n",
    "\n",
    "            # Create environment\n",
    "            env = SymbolicSatEnv(\n",
    "                formula=formula_to_use, reward_mode=\"dense\", max_steps=100\n",
    "            )\n",
    "            super().__init__(env)\n",
    "\n",
    "            # Get observation dimensions\n",
    "            obs, _ = env.reset()\n",
    "            processed_obs = preprocess_observation(obs)\n",
    "\n",
    "            # Define new observation space\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=-1.0, high=1.0, shape=(len(processed_obs),), dtype=np.float32\n",
    "            )\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "            return preprocess_observation(obs), info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            processed_obs = preprocess_observation(obs)\n",
    "            return processed_obs, reward, terminated, truncated, info\n",
    "\n",
    "    # Initialize Ray\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    # Configure the algorithm\n",
    "    config = (\n",
    "        DQNConfig()\n",
    "        .environment(env=SATGymWrapperRLlib, env_config={\"formula\": formula})\n",
    "        .training(gamma=0.99, lr=0.001, train_batch_size=64)\n",
    "        .exploration(\n",
    "            exploration_config={\n",
    "                \"initial_epsilon\": 0.9,\n",
    "                \"final_epsilon\": 0.05,\n",
    "                \"epsilon_timesteps\": 10000,\n",
    "            }\n",
    "        )\n",
    "        .resources(num_gpus=0)\n",
    "    )\n",
    "\n",
    "    print(\"Training Ray RLlib DQN agent...\")\n",
    "\n",
    "    # Train the model\n",
    "    stop = {\"training_iteration\": 100}\n",
    "    results = tune.run(\n",
    "        \"DQN\",\n",
    "        config=config.to_dict(),\n",
    "        stop=stop,\n",
    "        checkpoint_at_end=True,\n",
    "        checkpoint_freq=10,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Get the best trial\n",
    "    best_trial = results.get_best_trial(\"episode_reward_mean\", \"max\")\n",
    "    print(f\"Best trial: {best_trial.trial_id}\")\n",
    "    print(f\"Best trial final reward: {best_trial.last_result['episode_reward_mean']}\")\n",
    "\n",
    "    # Cleanup Ray\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098b97d",
   "metadata": {},
   "source": [
    "## 6. Compare Performance\n",
    "\n",
    "Let's compare the performance of all three implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent_type, agent, env, num_episodes=20):\n",
    "    \"\"\"Evaluate an agent on the environment.\"\"\"\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    solved = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if agent_type == \"custom\":\n",
    "            # Reset environment\n",
    "            obs, _ = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "            # Episode variables\n",
    "            total_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                episode_steps += 1\n",
    "                # Select action without exploration\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = (\n",
    "                        torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                    )\n",
    "                    q_values = agent.policy_net(state_tensor)\n",
    "                    action = q_values.max(1)[1].item()\n",
    "\n",
    "                # Take action\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "                next_state = preprocess_observation(next_obs)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Update state and metrics\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Check if solved\n",
    "                if done and info.get(\"solved\", False):\n",
    "                    solved += 1\n",
    "\n",
    "            # Save metrics\n",
    "            rewards.append(total_reward)\n",
    "            steps.append(episode_steps)\n",
    "\n",
    "        elif agent_type == \"sb3\" and sb3_available:\n",
    "            # Reset environment\n",
    "            obs, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                episode_steps += 1\n",
    "                # Select action\n",
    "                action, _ = agent.predict(obs, deterministic=True)\n",
    "\n",
    "                # Take action\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Update metrics\n",
    "                total_reward += reward\n",
    "\n",
    "                # Check if solved\n",
    "                if done and info.get(\"solved\", False):\n",
    "                    solved += 1\n",
    "\n",
    "            # Save metrics\n",
    "            rewards.append(total_reward)\n",
    "            steps.append(episode_steps)\n",
    "\n",
    "    return {\n",
    "        \"mean_reward\": np.mean(rewards),\n",
    "        \"std_reward\": np.std(rewards),\n",
    "        \"mean_steps\": np.mean(steps),\n",
    "        \"std_steps\": np.std(steps),\n",
    "        \"solved_ratio\": solved / num_episodes,\n",
    "    }\n",
    "\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluate custom DQN\n",
    "print(\"\\nEvaluating Custom PyTorch DQN...\")\n",
    "results[\"custom\"] = evaluate_agent(\"custom\", custom_agent, env_custom)\n",
    "print(\n",
    "    f\"Custom DQN - Mean reward: {results['custom']['mean_reward']:.2f}, \"\n",
    "    f\"Solved: {results['custom']['solved_ratio'] * 100:.1f}%, \"\n",
    "    f\"Mean steps: {results['custom']['mean_steps']:.1f}\"\n",
    ")\n",
    "\n",
    "# Evaluate SB3 if available\n",
    "if sb3_available:\n",
    "    print(\"\\nEvaluating Stable-Baselines3 DQN...\")\n",
    "    results[\"sb3\"] = evaluate_agent(\"sb3\", model, env_sb3)\n",
    "    print(\n",
    "        f\"SB3 DQN - Mean reward: {results['sb3']['mean_reward']:.2f}, \"\n",
    "        f\"Solved: {results['sb3']['solved_ratio'] * 100:.1f}%, \"\n",
    "        f\"Mean steps: {results['sb3']['mean_steps']:.1f}\"\n",
    "    )\n",
    "\n",
    "# For RLlib, we'd need a more complex evaluation approach which is omitted for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f47c86",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've demonstrated how to integrate SymbolicGym with different reinforcement learning libraries:\n",
    "\n",
    "1. **Custom PyTorch DQN**: Provides maximum flexibility but requires more manual implementation\n",
    "2. **Stable-Baselines3**: Offers a good balance of simplicity and performance with minimal custom code\n",
    "3. **Ray RLlib**: Powerful for distributed training and hyperparameter tuning\n",
    "\n",
    "The key steps for integration with any RL library are:\n",
    "1. Create a wrapper to convert dictionary observations to flat arrays\n",
    "2. Define appropriate observation and action spaces\n",
    "3. Configure the RL algorithm to match the SAT environment characteristics\n",
    "\n",
    "Each library has its strengths:\n",
    "- Custom implementation gives complete control over the algorithm\n",
    "- Stable-Baselines3 provides well-tested implementations for quick experimentation\n",
    "- RLlib excels at scaling to large distributed training\n",
    "\n",
    "Choose the approach that best fits your specific requirements for solving SAT problems with reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satrlgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
