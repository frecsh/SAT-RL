{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bd2186",
   "metadata": {},
   "source": [
    "# SymbolicGym Quickstart Guide\n",
    "\n",
    "This notebook provides a complete walkthrough of how to use SymbolicGym for training reinforcement learning agents to solve SAT problems. It covers:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Creating and configuring SAT problems\n",
    "3. Building and training a simple agent\n",
    "4. Evaluating performance\n",
    "5. Visualizing results\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, let's import the necessary libraries and check the SymbolicGym installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54db32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque\n",
    "import symbolicgym\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Disable PyTorch JIT compilation to avoid circular import issues\n",
    "os.environ['PYTORCH_JIT'] = '0'\n",
    "\n",
    "# Import PyTorch after setting the environment variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Check SymbolicGym version\n",
    "print(f\"Using SymbolicGym version: {getattr(symbolicgym, '__version__', 'dev')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165a77f",
   "metadata": {},
   "source": [
    "## 1. Creating SAT Problems\n",
    "\n",
    "SymbolicGym provides utilities for creating or loading SAT problems in CNF (Conjunctive Normal Form). We'll create a simple problem directly and also show how to load from DIMACS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28136d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load problems from CNF files\n",
    "from symbolicgym.utils.cnf import load_cnf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CNF file path (update as needed for your project structure)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "cnf_file = os.path.join(project_root, \"data\", \"benchmarks\",  \"uf50-01.cnf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03125737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple SAT problem\n",
    "simple_formula = {\n",
    "    \"clauses\": [[1, 2, 3], [-1, -2], [2, -3], [1, -3]],\n",
    "    \"num_vars\": 3,\n",
    "    \"name\": \"simple_test_formula\"\n",
    "}\n",
    "\n",
    "print(\"Formula structure:\")\n",
    "print(f\"Number of variables: {simple_formula['num_vars']}\")\n",
    "print(f\"Number of clauses: {len(simple_formula['clauses'])}\")\n",
    "print(\"Clauses:\")\n",
    "for i, clause in enumerate(simple_formula['clauses']):\n",
    "    print(f\"  Clause {i+1}: {clause}\")\n",
    "\n",
    "# Try to load a CNF file if available\n",
    "try:\n",
    "    example_cnf_path = cnf_file\n",
    "    if os.path.exists(example_cnf_path):\n",
    "        cnf_data = load_cnf_file(example_cnf_path)\n",
    "        loaded_formula = {\n",
    "            \"clauses\": cnf_data[\"clauses\"],\n",
    "            \"num_vars\": cnf_data[\"num_vars\"],\n",
    "            \"name\": os.path.basename(example_cnf_path)\n",
    "        }\n",
    "        print(f\"\\nLoaded formula from {example_cnf_path}\")\n",
    "        print(f\"Number of variables: {loaded_formula['num_vars']}\")\n",
    "        print(f\"Number of clauses: {len(loaded_formula['clauses'])}\")\n",
    "        complex_formula = loaded_formula\n",
    "    else:\n",
    "        print(\"\\nExample CNF file not found. Using only the manually created formula.\")\n",
    "        complex_formula = simple_formula\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading example file: {e}\")\n",
    "    print(\"Continuing with manually created formula only.\")\n",
    "    complex_formula = simple_formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e63bc6",
   "metadata": {},
   "source": [
    "## 2. Creating and Configuring the Environment\n",
    "\n",
    "Now we'll set up the SymbolicGym environment with our SAT problem. We'll explore different configurations including reward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment with the simple formula\n",
    "from symbolicgym.envs.sat import SymbolicSatEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb804e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of variables from the dictionary\n",
    "if isinstance(complex_formula['num_vars'], dict) and 'num_variables' in complex_formula['num_vars']:\n",
    "    num_variables = complex_formula['num_vars']['num_variables']\n",
    "    corrected_formula = {\n",
    "        'clauses': complex_formula['clauses'],\n",
    "        'num_vars': num_variables,\n",
    "        'name': complex_formula['name']\n",
    "    }\n",
    "else:\n",
    "    corrected_formula = complex_formula\n",
    "\n",
    "# Create environment with default settings (sparse rewards)\n",
    "env = SymbolicSatEnv(formula=corrected_formula)\n",
    "\n",
    "# Print environment information\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "\n",
    "# Reset the environment\n",
    "obs, info = env.reset(seed=RANDOM_SEED)\n",
    "print(\"\\nInitial observation:\")\n",
    "print(f\"Variables: {obs['variables']}\")\n",
    "print(f\"Clauses: {obs['clauses']}\")\n",
    "print(f\"Variable Assignment: {obs['variable_assignment']}\")\n",
    "print(f\"Clause Satisfaction: {obs['clause_satisfaction']}\")\n",
    "print(f\"Initial info: {info}\")\n",
    "\n",
    "# Let's try another environment with dense rewards\n",
    "env_dense = SymbolicSatEnv(formula=simple_formula, reward_mode=\"dense\")\n",
    "obs_dense, info_dense = env_dense.reset(seed=RANDOM_SEED)\n",
    "print(\"\\nEnvironment with dense rewards initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7613bfd",
   "metadata": {},
   "source": [
    "## 3. Interacting with the Environment\n",
    "\n",
    "Let's interact with the environment manually to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a few random actions\n",
    "print(\"Taking random actions:\")\n",
    "env.reset(seed=RANDOM_SEED)\n",
    "\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"\\nStep {i+1}, Action: {action} (flipping variable {action+1})\")\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"New variable values: {obs['variables']}\")\n",
    "    print(f\"Clause satisfaction: {obs['clause_satisfaction']}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Info: {info}\")\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished!\")\n",
    "        break\n",
    "\n",
    "# Now let's try with dense rewards\n",
    "print(\"\\n\\nTrying with dense rewards:\")\n",
    "env_dense.reset(seed=RANDOM_SEED)\n",
    "for i in range(5):\n",
    "    action = env_dense.action_space.sample()\n",
    "    print(f\"\\nStep {i+1}, Action: {action} (flipping variable {action+1})\")\n",
    "    obs, reward, terminated, truncated, info = env_dense.step(action)\n",
    "    print(f\"Reward (dense): {reward}\")\n",
    "    print(f\"Clause satisfaction: {obs['clause_satisfaction']}\")\n",
    "    print(f\"Info: {info}\")\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce6a30",
   "metadata": {},
   "source": [
    "## 4. Building a Simple DQN Agent\n",
    "\n",
    "Now let's build a simple Deep Q-Network agent to solve our SAT problem. This is a basic implementation to demonstrate how RL can be applied to SAT problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=64, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, buffer_size=10000, batch_size=64):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.max(1)[1].item()\n",
    "        else:\n",
    "            return random.randrange(self.action_size)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        # Convert lists to numpy arrays first for better performance\n",
    "        states = np.array(batch[0])\n",
    "        actions = np.array(batch[1])\n",
    "        rewards = np.array(batch[2])\n",
    "        next_states = np.array(batch[3])\n",
    "        dones = np.array(batch[4])\n",
    "        \n",
    "        # Then convert numpy arrays to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).view(-1, 1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        next_q = self.target_net(next_states).max(1)[0].detach()\n",
    "        expected_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(current_q, expected_q.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f31f5d",
   "metadata": {},
   "source": [
    "## 5. Training the Agent\n",
    "\n",
    "Now we'll train our DQN agent on the SAT problem. We'll use a simple preprocessing function to convert our environment observations into a format suitable for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "def preprocess_state(observation):\n",
    "    # Extract and flatten the variables and clauses arrays\n",
    "    variables = observation['variables']\n",
    "    clauses = observation['clauses']\n",
    "    \n",
    "    # Concatenate them into a single state vector\n",
    "    state = np.concatenate([variables, clauses])\n",
    "    return state\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100\n",
    "target_update = 10\n",
    "max_steps = 100\n",
    "env = SymbolicSatEnv(formula=corrected_formula, reward_mode=\"dense\", max_steps=max_steps)\n",
    "\n",
    "# Get dimensions for our agent\n",
    "obs, _ = env.reset()\n",
    "state = preprocess_state(obs)\n",
    "state_size = len(state)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Create our agent\n",
    "agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    hidden_size=64,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "rewards_history = []\n",
    "solved_episodes = 0\n",
    "clause_satisfaction_history = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = preprocess_state(obs)\n",
    "    \n",
    "    total_reward = 0\n",
    "    max_satisfaction = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select and perform action\n",
    "        action = agent.select_action(state)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = preprocess_state(next_obs)\n",
    "        \n",
    "        # Store transition\n",
    "        agent.memory.push(state, action, reward, next_state, terminated or truncated)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Track maximum clause satisfaction\n",
    "        current_satisfaction = info['satisfaction_ratio']\n",
    "        max_satisfaction = max(max_satisfaction, current_satisfaction)\n",
    "        \n",
    "        # Learn\n",
    "        loss = agent.learn()\n",
    "        \n",
    "        # If episode is done, break\n",
    "        if terminated or truncated:\n",
    "            if info.get('solved', False):\n",
    "                solved_episodes += 1\n",
    "            break\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        agent.update_target_net()\n",
    "    \n",
    "    # Update epsilon for exploration\n",
    "    agent.update_epsilon()\n",
    "    \n",
    "    # Save metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    clause_satisfaction_history.append(max_satisfaction)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "              f\"Total Reward: {total_reward:.2f}, \"\n",
    "              f\"Max Satisfaction: {max_satisfaction:.2f}, \"\n",
    "              f\"Epsilon: {agent.epsilon:.2f}, \"\n",
    "              f\"Solved Episodes: {solved_episodes}\")\n",
    "\n",
    "print(f\"\\nTraining completed. Solved {solved_episodes}/{num_episodes} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a05e37",
   "metadata": {},
   "source": [
    "## 6. Evaluating Agent Performance\n",
    "\n",
    "Let's evaluate the trained agent and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set agent to evaluation mode (no exploration)\n",
    "agent.epsilon = 0\n",
    "\n",
    "# Evaluation parameters\n",
    "num_eval_episodes = 20\n",
    "max_eval_steps = 100\n",
    "\n",
    "# Metrics\n",
    "eval_rewards = []\n",
    "eval_solved = 0\n",
    "eval_steps = []\n",
    "eval_satisfaction_ratios = []\n",
    "\n",
    "# Run evaluation episodes\n",
    "for episode in range(num_eval_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = preprocess_state(obs)\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps_taken = 0\n",
    "    max_satisfaction = 0\n",
    "    \n",
    "    for step in range(max_eval_steps):\n",
    "        steps_taken += 1\n",
    "        \n",
    "        # Select action without exploration\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "            q_values = agent.policy_net(state_tensor)\n",
    "            action = q_values.max(1)[1].item()\n",
    "        \n",
    "        # Take action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = preprocess_state(next_obs)\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        current_satisfaction = info['satisfaction_ratio']\n",
    "        max_satisfaction = max(max_satisfaction, current_satisfaction)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if terminated or truncated:\n",
    "            if info.get('solved', False):\n",
    "                eval_solved += 1\n",
    "            break\n",
    "    \n",
    "    # Save metrics\n",
    "    eval_rewards.append(total_reward)\n",
    "    eval_steps.append(steps_taken)\n",
    "    eval_satisfaction_ratios.append(max_satisfaction)\n",
    "    \n",
    "    print(f\"Evaluation Episode {episode + 1}: \"\n",
    "          f\"Reward: {total_reward:.2f}, \"\n",
    "          f\"Steps: {steps_taken}, \"\n",
    "          f\"Max Satisfaction: {max_satisfaction:.2f}, \"\n",
    "          f\"Solved: {info.get('solved', False)}\")\n",
    "\n",
    "print(f\"\\nEvaluation completed. \"\n",
    "      f\"Solved: {eval_solved}/{num_eval_episodes} episodes. \"\n",
    "      f\"Average Reward: {np.mean(eval_rewards):.2f}, \"\n",
    "      f\"Average Steps: {np.mean(eval_steps):.2f}, \"\n",
    "      f\"Average Satisfaction: {np.mean(eval_satisfaction_ratios):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ec4ee",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Results\n",
    "\n",
    "Finally, let's visualize our training and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a995604",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training rewards\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rewards_history, label='Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Rewards per Episode')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot clause satisfaction during training\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(clause_satisfaction_history, label='Max Clause Satisfaction', color='orange')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Satisfaction Ratio')\n",
    "plt.title('Max Clause Satisfaction per Episode')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot evaluation rewards\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(range(num_eval_episodes), eval_rewards, color='green')\n",
    "plt.xlabel('Evaluation Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation Rewards per Episode')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot steps to solve during evaluation\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(range(num_eval_episodes), eval_satisfaction_ratios, color='purple')\n",
    "plt.xlabel('Evaluation Episode')\n",
    "plt.ylabel('Max Satisfaction Ratio')\n",
    "plt.title('Evaluation Satisfaction Ratios')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28711d",
   "metadata": {},
   "source": [
    "## 8. Advanced Features: Using Oracle Guidance\n",
    "\n",
    "SymbolicGym allows integration with traditional SAT solver oracles to guide the learning process. Let's see how to leverage this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5355f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import oracle components\n",
    "from symbolicgym.oracles.simple_oracle import SimpleDPLLOracle\n",
    "from symbolicgym.oracles.oracle_protocol import OracleQuery\n",
    "\n",
    "# Create a simple oracle\n",
    "try:\n",
    "    oracle = SimpleDPLLOracle(\n",
    "        clauses=simple_formula[\"clauses\"],\n",
    "        num_vars=simple_formula[\"num_vars\"]\n",
    "    )\n",
    "    \n",
    "    # Reset the environment\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Create an oracle query\n",
    "    query = OracleQuery(\n",
    "        query_type=\"next_variable\",\n",
    "        current_assignment=obs[\"variable_assignment\"],\n",
    "        options={}\n",
    "    )\n",
    "    \n",
    "    # Get oracle guidance\n",
    "    oracle_response = oracle.query(query)\n",
    "    \n",
    "    print(\"\\nOracle Guidance:\")\n",
    "    print(f\"Response: {oracle_response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error using oracle: {e}\")\n",
    "    print(\"This section requires proper oracle implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061db83",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this tutorial, we've explored:\n",
    "\n",
    "1. Creating and configuring SAT environments in SymbolicGym\n",
    "2. Implementing a DQN agent for solving SAT problems\n",
    "3. Training and evaluating the agent\n",
    "4. Visualizing training and evaluation metrics\n",
    "5. Using oracle guidance for enhanced learning\n",
    "\n",
    "This is just a starting point for using reinforcement learning to tackle SAT problems. More advanced techniques like graph neural networks, MCTS, and specialized reward shaping can significantly improve performance.\n",
    "\n",
    "For more information, check out the SymbolicGym documentation or refer to the source code repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satrlgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
